{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09061317-ea52-4ab3-9769-379a4a1d4e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# import utils\n",
    "# from net import Net, Vgg16\n",
    "# from option import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56364d85-0ef0-4f1b-9c67-0c47009b8d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_load_rgbimage(filename, size=None, scale=None, keep_asp=False):\n",
    "    img = Image.open(filename).convert('RGB')\n",
    "    if size is not None:\n",
    "        if keep_asp:\n",
    "            size2 = int(size * 1.0 / img.size[0] * img.size[1])\n",
    "            img = img.resize((size, size2), Image.ANTIALIAS)\n",
    "        else:\n",
    "            img = img.resize((size, size), Image.ANTIALIAS)\n",
    "\n",
    "    elif scale is not None:\n",
    "        img = img.resize((int(img.size[0] / scale), int(img.size[1] / scale)), Image.ANTIALIAS)\n",
    "    img = np.array(img).transpose(2, 0, 1)\n",
    "    img = torch.from_numpy(img).float()\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ee1520b-8375-4de8-bb1d-5b370e88007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(batch):\n",
    "    batch = batch.transpose(0, 1)\n",
    "    (r, g, b) = torch.chunk(batch, 3)\n",
    "    batch = torch.cat((b, g, r))\n",
    "    batch = batch.transpose(0, 1)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb1d5577-faf9-4a2f-9748-a828b9d10bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image_file = \"./dancing.jpg\"\n",
    "content_size = 128\n",
    "\n",
    "style_image_file = \"./picasso.jpg\"\n",
    "style_size = 512\n",
    "\n",
    "content_image = tensor_load_rgbimage(content_image_file, size=content_size, keep_asp=True)\n",
    "content_image = content_image.unsqueeze(0)\n",
    "\n",
    "style = tensor_load_rgbimage(style_image_file, size=style_size)\n",
    "style = style.unsqueeze(0)\n",
    "style = preprocess_batch(style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "644c805c-5bfd-4691-b495-48ef64f2d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def var(x, dim=0):\n",
    "    '''\n",
    "    Calculates variance.\n",
    "    '''\n",
    "    x_zero_meaned = x - x.mean(dim).expand_as(x)\n",
    "    return x_zero_meaned.pow(2).mean(dim)\n",
    "\n",
    "\n",
    "class MultConst(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return 255*input\n",
    "\n",
    "\n",
    "class GramMatrix(nn.Module):\n",
    "    def forward(self, y):\n",
    "        (b, ch, h, w) = y.size()\n",
    "        features = y.view(b, ch, w * h)\n",
    "        features_t = features.transpose(1, 2)\n",
    "        gram = features.bmm(features_t) / (ch * h * w)\n",
    "        return gram\n",
    "    \n",
    "\n",
    "class Basicblock(nn.Module):\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, norm_layer=nn.BatchNorm2d):\n",
    "        super(Basicblock, self).__init__()\n",
    "        self.downsample = downsample\n",
    "        if self.downsample is not None:\n",
    "            self.residual_layer = nn.Conv2d(inplanes, planes,\n",
    "                                                        kernel_size=1, stride=stride)\n",
    "        conv_block=[]\n",
    "        conv_block+=[norm_layer(inplanes),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                ConvLayer(inplanes, planes, kernel_size=3, stride=stride),\n",
    "                                norm_layer(planes),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                ConvLayer(planes, planes, kernel_size=3, stride=1),\n",
    "                                norm_layer(planes)]\n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if self.downsample is not None:\n",
    "            residual = self.residual_layer(input)\n",
    "        else:\n",
    "            residual = input\n",
    "        return residual + self.conv_block(input)\n",
    "            \n",
    "\n",
    "class UpBasicblock(nn.Module):\n",
    "    \"\"\" Up-sample residual block (from MSG-Net paper)\n",
    "    Enables passing identity all the way through the generator\n",
    "    ref https://arxiv.org/abs/1703.06953\n",
    "    \"\"\"\n",
    "    def __init__(self, inplanes, planes, stride=2, norm_layer=nn.BatchNorm2d):\n",
    "        super(UpBasicblock, self).__init__()\n",
    "        self.residual_layer = UpsampleConvLayer(inplanes, planes,\n",
    "                                                      kernel_size=1, stride=1, upsample=stride)\n",
    "        conv_block=[]\n",
    "        conv_block+=[norm_layer(inplanes),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                UpsampleConvLayer(inplanes, planes, kernel_size=3, stride=1, upsample=stride),\n",
    "                                norm_layer(planes),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                ConvLayer(planes, planes, kernel_size=3, stride=1)]\n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.residual_layer(input) + self.conv_block(input)\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    \"\"\" Pre-activation residual block\n",
    "    Identity Mapping in Deep Residual Networks\n",
    "    ref https://arxiv.org/abs/1603.05027\n",
    "    \"\"\"\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, norm_layer=nn.BatchNorm2d):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.expansion = 4\n",
    "        self.downsample = downsample\n",
    "        if self.downsample is not None:\n",
    "            self.residual_layer = nn.Conv2d(inplanes, planes * self.expansion,\n",
    "                                                        kernel_size=1, stride=stride)\n",
    "        conv_block = []\n",
    "        conv_block += [norm_layer(inplanes),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(inplanes, planes, kernel_size=1, stride=1)]\n",
    "        conv_block += [norm_layer(planes),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    ConvLayer(planes, planes, kernel_size=3, stride=stride)]\n",
    "        conv_block += [norm_layer(planes),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(planes, planes * self.expansion, kernel_size=1, stride=1)]\n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.downsample is not None:\n",
    "            residual = self.residual_layer(x)\n",
    "        else:\n",
    "            residual = x\n",
    "        return residual + self.conv_block(x)\n",
    "\n",
    "\n",
    "class UpBottleneck(nn.Module):\n",
    "    \"\"\" Up-sample residual block (from MSG-Net paper)\n",
    "    Enables passing identity all the way through the generator\n",
    "    ref https://arxiv.org/abs/1703.06953\n",
    "    \"\"\"\n",
    "    def __init__(self, inplanes, planes, stride=2, norm_layer=nn.BatchNorm2d):\n",
    "        super(UpBottleneck, self).__init__()\n",
    "        self.expansion = 4\n",
    "        self.residual_layer = UpsampleConvLayer(inplanes, planes * self.expansion,\n",
    "                                                      kernel_size=1, stride=1, upsample=stride)\n",
    "        conv_block = []\n",
    "        conv_block += [norm_layer(inplanes),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(inplanes, planes, kernel_size=1, stride=1)]\n",
    "        conv_block += [norm_layer(planes),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    UpsampleConvLayer(planes, planes, kernel_size=3, stride=1, upsample=stride)]\n",
    "        conv_block += [norm_layer(planes),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(planes, planes * self.expansion, kernel_size=1, stride=1)]\n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return  self.residual_layer(x) + self.conv_block(x)\n",
    "\n",
    "\n",
    "class ConvLayer(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        reflection_padding = int(np.floor(kernel_size / 2))\n",
    "        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.reflection_pad(x)\n",
    "        out = self.conv2d(out)\n",
    "        return out\n",
    "\n",
    "class UpsampleConvLayer(torch.nn.Module):\n",
    "    \"\"\"UpsampleConvLayer\n",
    "    Upsamples the input and then does a convolution. This method gives better results\n",
    "    compared to ConvTranspose2d.\n",
    "    ref: http://distill.pub/2016/deconv-checkerboard/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
    "        super(UpsampleConvLayer, self).__init__()\n",
    "        self.upsample = upsample\n",
    "        if upsample:\n",
    "            self.upsample_layer = torch.nn.Upsample(scale_factor=upsample)\n",
    "        self.reflection_padding = int(np.floor(kernel_size / 2))\n",
    "        if self.reflection_padding != 0:\n",
    "            self.reflection_pad = nn.ReflectionPad2d(self.reflection_padding)\n",
    "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.upsample:\n",
    "            x = self.upsample_layer(x)\n",
    "        if self.reflection_padding != 0:\n",
    "            x = self.reflection_pad(x)\n",
    "        out = self.conv2d(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Inspiration(nn.Module):\n",
    "    \"\"\" Inspiration Layer (from MSG-Net paper)\n",
    "    tuning the featuremap with target Gram Matrix\n",
    "    ref https://arxiv.org/abs/1703.06953\n",
    "    \"\"\"\n",
    "    def __init__(self, C, B=1):\n",
    "        super(Inspiration, self).__init__()\n",
    "        # B is equal to 1 or input mini_batch\n",
    "        self.weight = nn.Parameter(torch.Tensor(1,C,C), requires_grad=True)\n",
    "        # non-parameter buffer\n",
    "        self.G = Variable(torch.Tensor(B,C,C), requires_grad=True)\n",
    "        self.C = C\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.weight.data.uniform_(0.0, 0.02)\n",
    "\n",
    "    def setTarget(self, target):\n",
    "        self.G = target\n",
    "\n",
    "    def forward(self, X):\n",
    "        # input X is a 3D feature map\n",
    "        self.P = torch.bmm(self.weight.expand_as(self.G),self.G)\n",
    "        return torch.bmm(self.P.transpose(1,2).expand(X.size(0), self.C, self.C), X.view(X.size(0),X.size(1),-1)).view_as(X)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "            + 'N x ' + str(self.C) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f86ddc98-c49b-4499-a0cc-b605bbe16695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vgg16(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vgg16, self).__init__()\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        h = F.relu(self.conv1_1(X))\n",
    "        h = F.relu(self.conv1_2(h))\n",
    "        relu1_2 = h\n",
    "        h = F.max_pool2d(h, kernel_size=2, stride=2)\n",
    "\n",
    "        h = F.relu(self.conv2_1(h))\n",
    "        h = F.relu(self.conv2_2(h))\n",
    "        relu2_2 = h\n",
    "        h = F.max_pool2d(h, kernel_size=2, stride=2)\n",
    "\n",
    "        h = F.relu(self.conv3_1(h))\n",
    "        h = F.relu(self.conv3_2(h))\n",
    "        h = F.relu(self.conv3_3(h))\n",
    "        relu3_3 = h\n",
    "        h = F.max_pool2d(h, kernel_size=2, stride=2)\n",
    "\n",
    "        h = F.relu(self.conv4_1(h))\n",
    "        h = F.relu(self.conv4_2(h))\n",
    "        h = F.relu(self.conv4_3(h))\n",
    "        relu4_3 = h\n",
    "\n",
    "        return [relu1_2, relu2_2, relu3_3, relu4_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "279f4242-da7f-4652-a174-6fa8fbf0ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_nc=3, output_nc=3, ngf=64, norm_layer=nn.InstanceNorm2d, n_blocks=6, gpu_ids=[]):\n",
    "        super(Net, self).__init__()\n",
    "        self.gpu_ids = gpu_ids\n",
    "        self.gram = GramMatrix()\n",
    "\n",
    "        block = Bottleneck\n",
    "        upblock = UpBottleneck\n",
    "        expansion = 4\n",
    "\n",
    "        model1 = []\n",
    "        model1 += [ConvLayer(input_nc, 64, kernel_size=7, stride=1),\n",
    "                            norm_layer(64),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            block(64, 32, 2, 1, norm_layer),\n",
    "                            block(32*expansion, ngf, 2, 1, norm_layer)]\n",
    "        self.model1 = nn.Sequential(*model1)\n",
    "\n",
    "        model = []\n",
    "        self.ins = Inspiration(ngf*expansion)\n",
    "        model += [self.model1]\n",
    "        model += [self.ins]    \n",
    "\n",
    "        for i in range(n_blocks):\n",
    "            model += [block(ngf*expansion, ngf, 1, None, norm_layer)]\n",
    "        \n",
    "        model += [upblock(ngf*expansion, 32, 2, norm_layer),\n",
    "                            upblock(32*expansion, 16, 2, norm_layer),\n",
    "                            norm_layer(16*expansion),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            ConvLayer(16*expansion, output_nc, kernel_size=7, stride=1)]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def setTarget(self, Xs):\n",
    "        F = self.model1(Xs)\n",
    "        G = self.gram(F)\n",
    "        self.ins.setTarget(G)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bced6818-dfb8-4dad-a91a-b1fdab06b52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngf = 128\n",
    "style_model = Net(ngf=ngf)\n",
    "\n",
    "model = \"models/21styles.model\"\n",
    "model_dict = torch.load(model)\n",
    "model_dict_clone = model_dict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ada2f0b-79f1-4ed3-bc91-642b78d6eace",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in model_dict_clone.items():\n",
    "    if key.endswith(('running_mean', 'running_var')):\n",
    "        del model_dict[key]\n",
    "style_model.load_state_dict(model_dict, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d30a7deb-e61e-41e3-a72b-40a791cf44bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if device == \"cuda\":\n",
    "    style_model.cuda()\n",
    "    content_image = content_image.cuda()\n",
    "    style = style.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "80d2aef2-4f3f-48da-832e-866913eae00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_save_rgbimage(tensor, filename, cuda=False):\n",
    "    if cuda:\n",
    "        img = tensor.clone().cpu().clamp(0, 255).numpy()\n",
    "    else:\n",
    "        img = tensor.clone().clamp(0, 255).numpy()\n",
    "    img = img.transpose(1, 2, 0).astype('uint8')\n",
    "    img = Image.fromarray(img)\n",
    "    img.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea16a260-1d23-4290-ac4b-2d6893a7116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_save_bgrimage(tensor, filename, cuda=False):\n",
    "    (b, g, r) = torch.chunk(tensor, 3)\n",
    "    tensor = torch.cat((r, g, b))\n",
    "    tensor_save_rgbimage(tensor, filename, cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc74f835-bc9a-45db-9e35-7f0117a4f1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    }
   ],
   "source": [
    "style_v = Variable(style)\n",
    "content_image = Variable(preprocess_batch(content_image))\n",
    "style_model.setTarget(style_v)\n",
    "output = style_model(content_image)\n",
    "\n",
    "#output = utils.color_match(output, style_v)\n",
    "\n",
    "output_image_file = \"./output.jpg\"\n",
    "tensor_save_bgrimage(output.data[0], output_image_file, True if device == \"cuda\" else False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch-1.0.0",
   "language": "python",
   "name": "pytorch-1.0.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
