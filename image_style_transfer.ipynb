{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4709355-13c6-4303-a743-50022ed3282f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 24.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: torchvision in /home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/site-packages\n",
      "Requirement already satisfied: numpy in /home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/site-packages (from torchvision)\n",
      "Requirement already satisfied: six in /home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/site-packages (from torchvision)\n",
      "Requirement already satisfied: torch in /home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/site-packages (from torchvision)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/site-packages (from torchvision)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 24.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/site-packages\n",
      "Requirement already satisfied: pytz in /home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/site-packages (from matplotlib)\n",
      "Requirement already satisfied: numpy>=1.7.1 in /home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/site-packages (from matplotlib)\n",
      "Requirement already satisfied: six>=1.10 in /home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/site-packages (from matplotlib)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/site-packages (from matplotlib)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/site-packages (from matplotlib)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/site-packages (from matplotlib)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/site-packages (from matplotlib)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 24.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fea5995d-8a3f-4c5b-b68f-98accd01097b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pillow==5.4.1\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/85/5e/e91792f198bbc5a0d7d3055ad552bc4062942d27eaf75c3e2783cf64eae5/Pillow-5.4.1-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.0MB 43.6MB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pillow\n",
      "  Found existing installation: Pillow 8.4.0\n",
      "    Uninstalling Pillow-8.4.0:\n",
      "      Successfully uninstalled Pillow-8.4.0\n",
      "Successfully installed pillow-5.4.1\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 24.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install pillow==5.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75b4b80f-0e33-4f82-b892-f56e7b1836fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c29c4953-6463-4ea3-b11e-fff407a50696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.4 |Anaconda, Inc.| (default, Mar 13 2018, 01:15:57) \n",
      "[GCC 7.2.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c989f856-1a17-4620-b377-8b96e74f6e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Feb 18 14:49:38 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  On   | 00000000:00:0D.0 Off |                    0 |\n",
      "| N/A   35C    P0    24W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "341764d0-956d-43f1-a7f8-bcefe98ce33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2017 NVIDIA Corporation\n",
      "Built on Fri_Sep__1_21:08:03_CDT_2017\n",
      "Cuda compilation tools, release 9.0, V9.0.176\n"
     ]
    }
   ],
   "source": [
    "!nvcc -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd14028e-43fa-4ac0-aae9-e05948579d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "img_size = (256, 256)\n",
    "print(f\"device is {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38991665-2aa9-42cf-86e0-7e94a3fb5623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(image_path):\n",
    "    pipeline = transforms.Compose(\n",
    "        [transforms.Resize((img_size)),\n",
    "         transforms.ToTensor()])\n",
    "\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    # img = Image.open(image_path)\n",
    "    img = pipeline(img).unsqueeze(0)\n",
    "    return img.to(device, torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21acc6e1-5169-495b-8ae9-ff546e47757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(tensor, image_path):\n",
    "    toPIL = transforms.ToPILImage()\n",
    "    img = tensor.detach().cpu().clone()\n",
    "    img = img.squeeze(0)\n",
    "    img = toPIL(img)\n",
    "    img.save(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67de53d8-e35b-4ba9-bb08-f770c1c93400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "style_img = read_image('./picasso.jpg')\n",
    "content_img = read_image('./dancing.jpg')\n",
    "\n",
    "default_content_layers = ['conv_4']\n",
    "default_style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "style_weight = 1e4\n",
    "content_weight = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef8bc7e1-f30f-4c4e-b06e-8d13baafd751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print(style_img.shape)\n",
    "print(content_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a337451-0b46-41bc-b359-7affa21802b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, target: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.loss = F.mse_loss(input, self.target)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3029405-c7cc-426b-a5df-7f6b2406de7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram(x: torch.Tensor):\n",
    "    # x is a [n, c, h, w] array\n",
    "    n, c, h, w = x.shape\n",
    "\n",
    "    features = x.reshape(n * c, h * w)\n",
    "    features = torch.mm(features, features.t()) / n / c / h / w\n",
    "    # features = torch.mm(features, features.T) / n / c / h / w\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f574fae8-ee3c-4233-952d-d2ea06e3d925",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, target: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.target = gram(target.detach()).detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        G = gram(input)\n",
    "        self.loss = F.mse_loss(G, self.target)\n",
    "        return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cffc09f4-f9b2-4815-9dab-479840557cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        super().__init__()\n",
    "        self.mean = torch.tensor(mean).to(device).reshape(-1, 1, 1)\n",
    "        self.std = torch.tensor(std).to(device).reshape(-1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return (img - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3bb86104-c12d-472b-b1f7-01099c826622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_losses(content_img, style_img, content_layers, style_layers):\n",
    "    num_loss = 0\n",
    "    expected_num_loss = len(content_layers) + len(style_layers)\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "\n",
    "    model = torch.nn.Sequential(\n",
    "        Normalization([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]))\n",
    "    cnn = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "    i = 0\n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, torch.nn.Conv2d):\n",
    "            i += 1\n",
    "            name = f'conv_{i}'\n",
    "        elif isinstance(layer, torch.nn.ReLU):\n",
    "            name = f'relu_{i}'\n",
    "            layer = torch.nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, torch.nn.MaxPool2d):\n",
    "            name = f'pool_{i}'\n",
    "        elif isinstance(layer, torch.nn.BatchNorm2d):\n",
    "            name = f'bn_{i}'\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                f'Unrecognized layer: {layer.__class__.__name__}')\n",
    "\n",
    "        model.add_module(name, layer)\n",
    "\n",
    "        if name in content_layers:\n",
    "            # add content loss:\n",
    "            target = model(content_img)\n",
    "            content_loss = ContentLoss(target)\n",
    "            model.add_module(f'content_loss_{i}', content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "            num_loss += 1\n",
    "\n",
    "        if name in style_layers:\n",
    "            target_feature = model(style_img)\n",
    "            style_loss = StyleLoss(target_feature)\n",
    "            model.add_module(f'style_loss_{i}', style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "            num_loss += 1\n",
    "\n",
    "        if num_loss >= expected_num_loss:\n",
    "            break\n",
    "\n",
    "    return model, content_losses, style_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28e64b3f-88ca-4a5e-ac30-59e29f1620ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "t() expects a 2D tensor, but self is 3D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-88a01b06ca19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: t() expects a 2D tensor, but self is 3D"
     ]
    }
   ],
   "source": [
    "# random_tensor = torch.rand(3, 4, 5)\n",
    "# print(random_tensor.shape)\n",
    "\n",
    "# print(random_tensor.t().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a22a0ef-60fc-4f04-a232-9f552254266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = torch.randn(1, 3, *img_size, device=device)\n",
    "model, content_losses, style_losses = get_model_and_losses(\n",
    "    content_img, style_img, default_content_layers, default_style_layers)\n",
    "\n",
    "input_img.requires_grad_(True)\n",
    "\n",
    "# model.requires_grad_(False)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6bbf752-96ea-4bb6-9cad-3dec37be7418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50:\n",
      "Loss: 2.7176737785339355\n",
      "Step 100:\n",
      "Loss: 2.116436243057251\n",
      "Step 150:\n",
      "Loss: 1.924469232559204\n",
      "Step 200:\n",
      "Loss: 1.8613355159759521\n",
      "Step 250:\n",
      "Loss: 1.8342278003692627\n",
      "Step 300:\n",
      "Loss: 1.8178861141204834\n",
      "Step 350:\n",
      "Loss: 1.8142468929290771\n",
      "Step 400:\n",
      "Loss: 2.65592622756958\n",
      "Step 450:\n",
      "Loss: 1.8093419075012207\n",
      "Step 500:\n",
      "Loss: 2.366575002670288\n",
      "Step 550:\n",
      "Loss: 1.8306089639663696\n",
      "Step 600:\n",
      "Loss: 1.9394431114196777\n",
      "Step 650:\n",
      "Loss: 1.9141770601272583\n",
      "Step 700:\n",
      "Loss: 228713.078125\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.LBFGS([input_img])\n",
    "steps = 0\n",
    "prev_loss = 0\n",
    "while steps <= 1000 and prev_loss < 100:\n",
    "\n",
    "    def closure():\n",
    "        with torch.no_grad():\n",
    "            input_img.clamp_(0, 1)\n",
    "        global steps\n",
    "        global prev_loss\n",
    "        optimizer.zero_grad()\n",
    "        model(input_img)\n",
    "        content_loss = 0\n",
    "        style_loss = 0\n",
    "        for ls in content_losses:\n",
    "            content_loss += ls.loss\n",
    "        for ls in style_losses:\n",
    "            style_loss += ls.loss\n",
    "        loss = content_weight * content_loss + style_weight * style_loss\n",
    "        loss.backward()\n",
    "        steps += 1\n",
    "        if steps % 50 == 0:\n",
    "            print(f'Step {steps}:')\n",
    "            print(f'Loss: {loss}')\n",
    "            save_image(input_img, f'work_dirs/output_{steps}.jpg')\n",
    "        prev_loss = loss\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09061317-ea52-4ab3-9769-379a4a1d4e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    input_img.clamp_(0, 1)\n",
    "save_image(input_img, 'work_dirs/output.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch-1.0.0",
   "language": "python",
   "name": "pytorch-1.0.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
